{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Pyolite",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Machine Learning Foundation\n\n## Section 2, Part c: Cross Validation\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Learning objectives\n\nBy the end of this lesson, you will be able to:\n\n*   Chain multiple data processing steps together using `Pipeline`\n*   Use the `KFolds` object to split data into multiple folds.\n*   Perform cross validation using SciKit Learn with `cross_val_predict` and `GridSearchCV`\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import piplite\nawait piplite.install(['tqdm', 'seaborn', 'skillsnetwork', 'pandas', 'numpy', 'scikit-learn'])",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Surpress warnings:\ndef warn(*args, **kwargs):\n    pass\nimport warnings\nwarnings.warn = warn\n\nimport numpy as np\nimport pickle\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport skillsnetwork\n\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.model_selection import KFold, cross_val_predict\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge\nfrom sklearn.metrics import r2_score\nfrom sklearn.pipeline import Pipeline",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-19T17:20:55.865735Z",
          "start_time": "2019-02-19T17:20:54.685698Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Note we are loading a slightly different (\"cleaned\") pickle file\n\nURL = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML240EN-SkillsNetwork/labs/data/boston_housing_clean.pickle'\nawait skillsnetwork.download_dataset(URL)\n\nboston = pickle.load(open('boston_housing_clean.pickle', \"rb\" ))\n",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "boston.keys()",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-19T17:21:01.676948Z",
          "start_time": "2019-02-19T17:21:01.671315Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "boston_data = boston['dataframe']\nboston_description = boston['description']",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-19T17:21:10.682135Z",
          "start_time": "2019-02-19T17:21:10.678835Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "boston_data.head()",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-19T17:22:10.274974Z",
          "start_time": "2019-02-19T17:22:10.256879Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Discussion:\n\nSuppose we want to do Linear Regression on our dataset to get an estimate, based on mean squared error, of how well our model will perform on data outside our dataset.\n\nSuppose also that our data is split into three folds: Fold 1, Fold 2, and Fold 3.\n\nWhat would the steps be, in English, to do this?\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**Your response below**\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "#### Coding this up\n\nThe [`KFold`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML240ENSkillsNetwork34171862-2022-01-01) object in SciKit Learn tells the cross validation object (see below) how to split up the data:\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "X = boston_data.drop('MEDV', axis=1)\ny = boston_data.MEDV",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-19T17:23:05.068445Z",
          "start_time": "2019-02-19T17:23:05.064683Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "kf = KFold(shuffle=True, random_state=72018, n_splits=3)",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-19T17:23:10.538982Z",
          "start_time": "2019-02-19T17:23:10.534325Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "for train_index, test_index in kf.split(X):\n    print(\"Train index:\", train_index[:10], len(train_index))\n    print(\"Test index:\",test_index[:10], len(test_index))\n    print('')",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-19T17:25:04.578536Z",
          "start_time": "2019-02-19T17:25:04.568959Z"
        },
        "scrolled": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "#from sklearn.metrics import r2_score, mean_squared_error\n\nscores = []\nlr = LinearRegression()\n\nfor train_index, test_index in kf.split(X):\n    X_train, X_test, y_train, y_test = (X.iloc[train_index, :], \n                                        X.iloc[test_index, :], \n                                        y[train_index], \n                                        y[test_index])\n    \n    lr.fit(X_train, y_train)\n        \n    y_pred = lr.predict(X_test)\n\n    score = r2_score(y_test.values, y_pred)\n    \n    scores.append(score)\n    \nscores",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-19T17:28:10.441616Z",
          "start_time": "2019-02-19T17:28:10.204857Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "A bit cumbersome, but do-able.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Discussion (Part 2):\n\nNow suppose we want to do the same, but appropriately scaling our data as we go through the folds.\n\nWhat would the steps be *now*?\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**Your response below**\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Coding this up\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "scores = []\n\nlr = LinearRegression()\ns = StandardScaler()\n\nfor train_index, test_index in kf.split(X):\n    X_train, X_test, y_train, y_test = (X.iloc[train_index, :], \n                                        X.iloc[test_index, :], \n                                        y[train_index], \n                                        y[test_index])\n    \n    X_train_s = s.fit_transform(X_train)\n    \n    lr.fit(X_train_s, y_train)\n    \n    X_test_s = s.transform(X_test)\n    \n    y_pred = lr.predict(X_test_s)\n\n    score = r2_score(y_test.values, y_pred)\n    \n    scores.append(score)",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-19T17:32:07.846336Z",
          "start_time": "2019-02-19T17:32:07.808810Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "scores",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-19T17:32:09.978972Z",
          "start_time": "2019-02-19T17:32:09.974341Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "(same scores, because for vanilla linear regression with no regularization, scaling actually doesn't matter for performance)\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "This is getting quite cumbersome!\n\n*Very* luckily, SciKit Learn has some wonderful functions that handle a lot of this for us.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### `Pipeline` and `cross_val_predict`\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "`Pipeline` lets you chain together multiple operators on your data that both have a `fit` method.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "s = StandardScaler()\nlr = LinearRegression()",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-19T17:33:28.695381Z",
          "start_time": "2019-02-19T17:33:28.691865Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Combine multiple processing steps into a `Pipeline`\n\nA pipeline contains a series of steps, where a step is (\"name of step\", actual_model). The \"name of step\" string is only used to help you identify which step you are on, and to allow you to specify parameters at that step.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "estimator = Pipeline([(\"scaler\", s),\n                      (\"regression\", lr)])",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-19T17:33:32.657281Z",
          "start_time": "2019-02-19T17:33:32.653852Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### `cross_val_predict`\n\n[`cross_val_predict`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_predict.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML240ENSkillsNetwork34171862-2022-01-01) is a function that does K-fold cross validation for us, appropriately fitting and transforming at every step of the way.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "kf",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "predictions = cross_val_predict(estimator, X, y, cv=kf)",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-19T17:35:55.255356Z",
          "start_time": "2019-02-19T17:35:55.240376Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "r2_score(y, predictions)",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-19T17:36:03.801732Z",
          "start_time": "2019-02-19T17:36:03.796646Z"
        },
        "scrolled": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "np.mean(scores) # almost identical!",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-19T17:36:19.831481Z",
          "start_time": "2019-02-19T17:36:19.827131Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Note that `cross_val_predict` doesn't use the same model for all steps; the predictions for each row are made when that row is in the validation set. We really have the collected results of 3 (i.e. `kf.num_splits`) different models.\n\nWhen we are done, `estimator` is still not fitted. If we want to predict on *new* data, we still have to train our `estimator`.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Hyperparameter tuning\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Definition\n\n**Hyperparameter tuning** involves using cross validation (or train-test split) to determine which hyperparameters are most likely to generate a model that *generalizes* well outside of your sample.\n\n### Mechanics\n\nWe can generate an exponentially spaces range of values using the numpy [`geomspace`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.geomspace.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML240ENSkillsNetwork34171862-2022-01-01#numpy.geomspace) function.\n\n```python\nnp.geomspace(1, 1000, num=4)\n```\n\nproduces:\n\n```\narray([    1.,    10.,   100.,  1000.])\n```\n\nUse this function to generate a list of length 10 called `alphas` for hyperparameter tuning:\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "alphas = np.geomspace(1e-9, 1e0, num=10)\nalphas",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-19T17:40:36.220744Z",
          "start_time": "2019-02-19T17:40:36.214714Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "The code below tunes the `alpha` hyperparameter for Lasso regression.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "scores = []\ncoefs = []\nfor alpha in alphas:\n    las = Lasso(alpha=alpha, max_iter=100000)\n    \n    estimator = Pipeline([\n        (\"scaler\", s),\n        (\"lasso_regression\", las)])\n\n    predictions = cross_val_predict(estimator, X, y, cv = kf)\n    \n    score = r2_score(y, predictions)\n    \n    scores.append(score)",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-19T17:47:34.947390Z",
          "start_time": "2019-02-19T17:47:34.845011Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "list(zip(alphas,scores))",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-19T17:47:35.397285Z",
          "start_time": "2019-02-19T17:47:35.390917Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "Lasso(alpha=1e-6).fit(X, y).coef_",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-19T17:48:16.943881Z",
          "start_time": "2019-02-19T17:48:16.937741Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "Lasso(alpha=1.0).fit(X, y).coef_",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-19T17:48:40.778026Z",
          "start_time": "2019-02-19T17:48:40.771732Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "plt.figure(figsize=(10,6))\nplt.semilogx(alphas, scores, '-o')\nplt.xlabel('$\\\\alpha$')\nplt.ylabel('$R^2$');",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-19T17:46:03.208768Z",
          "start_time": "2019-02-19T17:46:02.898338Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Exercise\n\nAdd `PolynomialFeatures` to this `Pipeline`, and re-run the cross validation with the `PolynomialFeatures` added.\n\n**Hint #1:** pipelines process input from first to last. Think about the order that it would make sense to add Polynomial Features to the data in sequence and add them in the appropriate place in the pipeline.\n\n**Hint #2:** you should see a significant increase in cross validation accuracy from doing this\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "pf = PolynomialFeatures(degree=3)\n\nscores = []\nalphas = np.geomspace(0.06, 6.0, 20)\nfor alpha in alphas:\n    las = Lasso(alpha=alpha, max_iter=100000)\n    \n    estimator = Pipeline([\n        (\"scaler\", s),\n        (\"make_higher_degree\", pf),\n        (\"lasso_regression\", las)])\n\n    predictions = cross_val_predict(estimator, X, y, cv = kf)\n    \n    score = r2_score(y, predictions)\n    \n    scores.append(score)\n    ",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-19T17:56:02.883875Z",
          "start_time": "2019-02-19T17:56:00.477593Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "If you store the results in a list called `scores`, the following will work:\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "plt.semilogx(alphas, scores);",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-19T17:56:05.154628Z",
          "start_time": "2019-02-19T17:56:04.987932Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Once we have found the hyperparameter (alpha~1e-2=0.01)\n# make the model and train it on ALL the data\n# Then release it into the wild .....\nbest_estimator = Pipeline([\n                    (\"scaler\", s),\n                    (\"make_higher_degree\", PolynomialFeatures(degree=2)),\n                    (\"lasso_regression\", Lasso(alpha=0.03))])\n\nbest_estimator.fit(X, y)\nbest_estimator.score(X, y)",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-19T17:56:40.051147Z",
          "start_time": "2019-02-19T17:56:40.009641Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "best_estimator.named_steps[\"lasso_regression\"].coef_",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-19T17:56:51.526585Z",
          "start_time": "2019-02-19T17:56:51.521094Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Exercise\n\nDo the same, but with `Ridge` regression\n\nWhich model, `Ridge` or `Lasso`, performs best with its optimal hyperparameters on the Boston dataset?\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "pf = PolynomialFeatures(degree=2)\nalphas = np.geomspace(4, 20, 20)\nscores=[]\nfor alpha in alphas:\n    ridge = Ridge(alpha=alpha, max_iter=100000)\n\n    estimator = Pipeline([\n        (\"scaler\", s),\n        (\"polynomial_features\", pf),\n        (\"ridge_regression\", ridge)])\n\n    predictions = cross_val_predict(estimator, X, y, cv = kf)\n    score = r2_score(y, predictions)\n    scores.append(score)\n\nplt.plot(alphas, scores)",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-19T17:59:17.634414Z",
          "start_time": "2019-02-19T17:59:17.238684Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "**Conclusion:** Both Lasso and Ridge with proper hyperparameter tuning give better results than plain ol' Linear Regression!\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Exercise:\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Now, for whatever your best overall hyperparameter was:\n\n*   Standardize the data\n*   Fit and predict on the entire dataset\n*   See what the largest coefficients were\n\n    *   Hint: use\n\n    ```python\n    dict(zip(model.coef_, pf.get_feature_names()))\n    ```\n\n    for your model `model` to get the feature names from `PolynomialFeatures`.\n\n    Then, use\n\n    ```python\n    dict(zip(list(range(len(X.columns.values))), X.columns.values))\n    ```\n\n    to see which features in the `PolynomialFeatures` DataFrame correspond to which columns in the original DataFrame.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Once we have found the hyperparameter (alpha~1e-2=0.01)\n# make the model and train it on ALL the data\n# Then release it into the wild .....\nbest_estimator = Pipeline([\n                    (\"scaler\", s),\n                    (\"make_higher_degree\", PolynomialFeatures(degree=2)),\n                    (\"lasso_regression\", Lasso(alpha=0.03))])\n\nbest_estimator.fit(X, y)\nbest_estimator.score(X, y)",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-19T17:56:40.051147Z",
          "start_time": "2019-02-19T17:56:40.009641Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "df_importances = pd.DataFrame(zip(best_estimator.named_steps[\"make_higher_degree\"].get_feature_names(),\n                 best_estimator.named_steps[\"lasso_regression\"].coef_,\n))",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-19T17:56:51.526585Z",
          "start_time": "2019-02-19T17:56:51.521094Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "col_names_dict = dict(zip(list(range(len(X.columns.values))), X.columns.values))",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "col_names_dict",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "df_importances.sort_values(by=1)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Grid Search CV\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "To do cross-validation, we used two techniques:\n\n*   use `KFolds` and manually create a loop to do cross-validation\n*   use `cross_val_predict` and `score` to get a cross-valiated score in a couple of lines.\n\nTo do hyper-parameter tuning, we see a general pattern:\n\n*   use `cross_val_predict` and `score` in a manually written loop over hyperparemeters, then select the best one.\n\nPerhaps not surprisingly, there is a function that does this for us -- `GridSearchCV`\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from sklearn.model_selection import GridSearchCV\n\n# Same estimator as before\nestimator = Pipeline([(\"scaler\", StandardScaler()),\n        (\"polynomial_features\", PolynomialFeatures()),\n        (\"ridge_regression\", Ridge())])\n\nparams = {\n    'polynomial_features__degree': [1, 2, 3],\n    'ridge_regression__alpha': np.geomspace(4, 20, 30)\n}\n\ngrid = GridSearchCV(estimator, params, cv=kf)",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-19T18:02:31.945804Z",
          "start_time": "2019-02-19T18:02:31.938416Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "grid.fit(X, y)",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-19T18:02:49.319148Z",
          "start_time": "2019-02-19T18:02:46.093880Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "grid.best_score_, grid.best_params_",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-19T18:03:07.016198Z",
          "start_time": "2019-02-19T18:03:07.010215Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "y_predict = grid.predict(X)",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-19T18:03:31.735568Z",
          "start_time": "2019-02-19T18:03:31.728658Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# This includes both in-sample and out-of-sample\nr2_score(y, y_predict)",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-19T18:04:17.838943Z",
          "start_time": "2019-02-19T18:04:17.832872Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Notice that \"grid\" is a fit object!\n# We can use grid.predict(X_test) to get brand new predictions!\ngrid.best_estimator_.named_steps['ridge_regression'].coef_",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-19T18:04:27.088854Z",
          "start_time": "2019-02-19T18:04:27.082915Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "grid.cv_results_",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-19T18:05:34.756588Z",
          "start_time": "2019-02-19T18:05:34.728508Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Summary\n\n1.  We can manually generate folds by using `KFolds`\n2.  We can get a score using `cross_val_predict(X, y, cv=KFoldObject_or_integer)`.\n    This will produce the out-of-bag prediction for each row.\n3.  When doing hyperparameter selection, we should be optimizing on out-of-bag scores. This means either using `cross_val_predict` in a loop, or ....\n4.  .... use `GridSearchCV`. GridSearchCV takes a model (or pipeline) and a dictionary of parameters to scan over. It finds the hyperparameter set that has the best out-of-sample score on all the parameters, and calls that it's \"best estimator\". It then retrains on all data with the \"best\" hyper-parameters.\n\n### Extensions\n\nHere are some additional items to keep in mind:\n\n*   There is a `RandomSearchCV` that tries random combination of model parameters. This can be helpful if you have a prohibitive number of combinations to test them all exhaustively.\n*   KFolds will randomly select rows to be in the training and test folds. There are other methods (such as `StratifiedKFolds` and `GroupKFold`, which are useful when you need more control over how the data is split (e.g. to prevent data leakage). You can create these specialized objects and pass them to the `cv` argument of `GridSearchCV`.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "***\n\n### Machine Learning Foundation (C) 2020 IBM Corporation\n",
      "metadata": {}
    }
  ]
}